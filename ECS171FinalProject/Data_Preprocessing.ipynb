{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Drop Columns"
      ],
      "metadata": {
        "id": "ooUKAlHLqFLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The purpose is to predict the delay, so Dataset \"airlines\" and \"airports\" will be process later as labels.\n",
        "\n",
        "#Clean up the main dataset \"flights\"\n",
        "columns = flights.columns\n",
        "print(\"Features of current dataset\",columns)\n",
        "\n",
        "#Clean up the non-related data\n",
        "#Drop the reason of delaying / Cancellation_reasons / cancelled / diverted etc.\n",
        "#Reason of dropping, please see README\n",
        "flights=flights.drop(['WEATHER_DELAY','LATE_AIRCRAFT_DELAY','AIRLINE_DELAY','SECURITY_DELAY','AIR_SYSTEM_DELAY','CANCELLATION_REASON',\n",
        " \"CANCELLED\", \"DIVERTED\",\"TAXI_IN\", \"TAXI_OUT\"],axis=1)\n",
        "\n",
        "columns = flights.columns\n",
        "print(\"Features after cleaning unrelated dataset\",columns)"
      ],
      "metadata": {
        "id": "U0k4UJbKqFrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean up the overlap numbers, reasons please see the README\n",
        "flights=flights.drop(['SCHEDULED_ARRIVAL','ARRIVAL_TIME','DEPARTURE_TIME','SCHEDULED_DEPARTURE','TAIL_NUMBER','FLIGHT_NUMBER', 'WHEELS_OFF', 'WHEELS_ON', \"SCHEDULED_TIME\", \"ELAPSED_TIME\"],axis=1)\n",
        "\n",
        "# Since all data is collected from 2015, year is meaningless to help prediction\n",
        "print(flights[\"YEAR\"].unique())\n",
        "flights=flights.drop(['YEAR'],axis=1)"
      ],
      "metadata": {
        "id": "LYKOwfNTqFyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = flights.columns\n",
        "print(\"Features after cleaning unrelated dataset\",columns)\n",
        "flights.shape"
      ],
      "metadata": {
        "id": "K8iIHs_YqF0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(flights.dtypes)\n",
        "flights"
      ],
      "metadata": {
        "id": "DaqOoAmBqF2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Categorial Feature"
      ],
      "metadata": {
        "id": "6oWG9yseqUOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_flights = flights.loc[:,['DEPARTURE_DELAY',\"AIR_TIME\",\"DISTANCE\"]]\n",
        "one_hot_original = flights.loc[:,['AIRLINE',\"DAY_OF_WEEK\",\"MONTH\"]]\n",
        "ordinal_orginal = flights.loc[:,['DAY',\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"]]\n",
        "\n",
        "display(one_hot_original)\n",
        "encoder = preprocessing.OneHotEncoder()\n",
        "encoder.fit(one_hot_original)\n",
        "one_hot = encoder.transform(one_hot_original)\n",
        "print(one_hot.shape)\n",
        "\n",
        "\n",
        "display(one_hot_original)\n",
        "one_hot_encode=one_hot_original.astype(\"str\")\n",
        "encoder = preprocessing.OrdinalEncoder()\n",
        "encoder.fit(one_hot_encode)\n",
        "ordinal_cat = encoder.transform(one_hot_encode)\n",
        "print(ordinal_cat, ordinal_cat.shape)\n",
        "\n",
        "display(ordinal_orginal)\n",
        "ordinal_orginal[\"DAY\"] = ordinal_orginal[\"DAY\"].astype(\"str\")\n",
        "ordinal_orginal.dtypes\n",
        "ordinal_orginal=ordinal_orginal.astype(\"str\")\n",
        "#ordinal_orginal.dtypes\n",
        "encoder = preprocessing.OrdinalEncoder()\n",
        "encoder.fit(ordinal_orginal)\n",
        "ordinal = encoder.transform(ordinal_orginal)\n",
        "print(ordinal, ordinal.shape)\n",
        "\n",
        "df_ordinal = pd.DataFrame(ordinal, columns = ['DAY','ORIGIN_AIRPORT','DESTINATION_AIRPORT'])\n",
        "df_cat = pd.DataFrame(ordinal_cat, columns = ['AIRLINE','DAY_OF_WEEK','MONTH'])\n",
        "\n",
        "df_flights = num_flights.join(df_ordinal)\n",
        "df_flights = df_flights.join(df_cat)\n",
        "df_flights"
      ],
      "metadata": {
        "id": "Y7gcrwdvqUhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Figure Display after Encoding"
      ],
      "metadata": {
        "id": "kjpC5rl2qiJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "corr = df_flights.corr()\n",
        "sns.heatmap(corr, annot = True, vmin=-1, vmax=1, center= 0,cmap= 'RdBu')"
      ],
      "metadata": {
        "id": "ZdPGs0pyqlR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data=df_flights.sample(n=8000),\n",
        "             vars=df_flights.columns,\n",
        "             diag_kind='kde',\n",
        "             plot_kws=dict(\n",
        "                 size=.5,\n",
        "                 alpha=.5,\n",
        "             ))"
      ],
      "metadata": {
        "id": "qLme3segqlV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaler"
      ],
      "metadata": {
        "id": "QwoiEJ_ZqiO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df_flights)\n",
        "df_flight_noscale =df_flights\n",
        "df_flights = scaler.transform (df_flights)\n",
        "\n",
        "# Make df_flights into Dataframe\n",
        "df_flights = pd.DataFrame(df_flights)\n",
        "df_flights = df_flights.rename(columns={0:'DEPARTURE_DELAY',1:'AIR_TIME',2:'DISTANCE',3:'DAY',4:'ORINGIN_AIRPORT',5:'DESTINATION_AIRPORT',6:'AIRLINES',7:'DAY_OF_WEEK',8:'MONTH' })\n",
        "\n",
        "target = flights[\"ARRIVAL_DELAY\"]\n",
        "df_flights = df_flights.join(target)\n",
        "df_flights"
      ],
      "metadata": {
        "id": "D16uS48vqqAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Clean up NAN Values"
      ],
      "metadata": {
        "id": "S6H5FJyXqimo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"DEPARTURE_DELAY\", \"AIR_TIME\"\n",
        "# When there is no departure_delay and arrival dealy, value would be None\n",
        "# Under this condition, set these NAN values as 0\n",
        "\n",
        "# check NAN values\n",
        "display(df_flights)\n",
        "display(df_flights.isna().any())\n",
        "\n",
        "df_flights['DEPARTURE_DELAY'] = df_flights['DEPARTURE_DELAY'].fillna(0)\n",
        "df_flights['ARRIVAL_DELAY'] = df_flights['ARRIVAL_DELAY'].fillna(0)\n",
        "\n",
        "print(\"After filling DEPARTURE_DELAY, and ARRIVAL_DELAY:\")\n",
        "display(df_flights.isna().any())"
      ],
      "metadata": {
        "id": "4-q2gmQYqUj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop rows where the air_time is NAN. (It is noise on Air_time record)\n",
        "# 0.18% of overall samples is dropped\n",
        "df_flights = df_flights.dropna()\n",
        "display(df_flights)\n",
        "print(\"after deleting all rows that contain NaN values,\")\n",
        "display(df_flights.isna().any())"
      ],
      "metadata": {
        "id": "DSRApe7gqUmR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}